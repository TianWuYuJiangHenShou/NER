{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# import pretraining_args as args\n",
    "from transformers import BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer,BertConfig,BertForTokenClassification,BertModel\n",
    "import time,datetime\n",
    "from sklearn.metrics import precision_score,classification_report,f1_score,recall_score\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import AlbertConfig, AlbertModel,AlbertForTokenClassification\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from crf import CRF\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/root/yy/data/ResumeNER'\n",
    "base_path = '../berts/albert_base_zh'\n",
    "train_path = 'train.char.bmes'\n",
    "dev_path = 'dev.char.bmes'\n",
    "test_path = 'test.char.bmes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base,train_path):\n",
    "    full = os.path.join(base,train_path)\n",
    "    with open(full,'r',encoding='utf-8')as f:\n",
    "        data = f.readlines()\n",
    "    tokens,labels = [],[]\n",
    "    token,label = [],[]\n",
    "    for line in data:\n",
    "        line= line.strip().replace(\"\\n\",'')\n",
    "        if len(line.split(' ')) == 2:\n",
    "            token.append(line.split(' ')[0])\n",
    "            label.append(line.split(' ')[1])\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "            token,label = [],[]\n",
    "    return tokens,labels\n",
    "\n",
    "def trans2id(labels):\n",
    "    tag_set = set()\n",
    "    for line in labels:\n",
    "        for label  in line:\n",
    "            if label not in tag_set:\n",
    "                tag_set.add(label)\n",
    "    tag_set.add('[CLS]')\n",
    "    tag_set.add('[SEP]')\n",
    "    tag_set = list(tag_set)\n",
    "    idx = [i for i in range(len(tag_set))]\n",
    "    tag2id = dict(zip(tag_set,idx))\n",
    "    id2tag = dict(zip(idx,tag_set))\n",
    "    return tag2id,id2tag\n",
    "\n",
    "def gen_features(tokens,labels,tokenizer,tag2id,max_len):\n",
    "    input_ids,tags,masks,lengths = [],[],[],[]\n",
    "    for i,(token,label) in enumerate(zip(tokens,labels)):\n",
    "        lengths.append(len(token))\n",
    "        if len(token) >= max_len - 2:\n",
    "            token = token[0:max_len - 2]\n",
    "            label = labels[i][0:max_len - 2]\n",
    "        mask = [1] * len(token)\n",
    "        \n",
    "        token = '[CLS] ' + ' '.join(token) + ' [SEP]'\n",
    "        tokenized_text = tokenizer.tokenize(token)\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        label = [tag2id['[CLS]']] + [tag2id[i] for i in label] + [tag2id['[SEP]']]\n",
    "        mask = [0] + mask + [0]\n",
    "        # padding\n",
    "        if len(input_id) < max_len:\n",
    "            input_id = input_id + [0] * (max_len - len(input_id))\n",
    "            label = label + [tag2id['O']] * (max_len - len(label))\n",
    "            mask = mask + [0] * (max_len - len(mask))\n",
    "        \n",
    "        assert len(input_id) == max_len\n",
    "        assert len(label) == max_len\n",
    "        assert len(mask) == max_len\n",
    "         \n",
    "        input_ids.append(input_id)\n",
    "        tags.append(label)\n",
    "        masks.append(mask)\n",
    "    return input_ids,tags,masks,lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "bs = 32\n",
    "tokenizer = BertTokenizer.from_pretrained('../berts/bert-base-transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tokens,train_labels = load_data(base,train_path)\n",
    "tag2id,id2tag = trans2id(train_labels)\n",
    "train_ids,train_tags,train_masks,train_lengths = gen_features(train_tokens,train_labels,tokenizer,tag2id,max_len)\n",
    "\n",
    "dev_tokens,dev_labels = load_data(base,dev_path)\n",
    "dev_ids,dev_tags,dev_masks,dev_lengths = gen_features(dev_tokens,dev_labels,tokenizer,tag2id,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ids = torch.tensor(train_ids)\n",
    "train_tags = torch.tensor(train_tags)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "# train_lengths = torch.tensor(train_lengths)\n",
    "\n",
    "dev_ids = torch.tensor(dev_ids)\n",
    "dev_tags = torch.tensor(dev_tags)\n",
    "dev_masks = torch.tensor(dev_masks)\n",
    "# dev_lengths = torch.tensor(dev_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_ids, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(dev_ids, dev_masks, dev_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_CRF(nn.Module):\n",
    "    def __init__(self,base_path,num_labels,lstm_hidden_size = 128,dropout = 0.3,lm_flag = False):\n",
    "        super(Bert_CRF,self).__init__()\n",
    "        bert_config = AlbertConfig.from_json_file(os.path.join(base_path,'config.json'))\n",
    "        bert_config.num_labels = num_labels\n",
    "        #hidden_states (tuple(torch.FloatTensor), optional, returned when config.output_hidden_states=True):\n",
    "        #方案二要加下面两个config\n",
    "        bert_config.output_hidden_states=True\n",
    "        bert_config.output_attentions=True\n",
    "        self.bert = AlbertModel.from_pretrained(os.path.join(base_path,'pytorch_model.bin'), config=bert_config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #lstm input_size = bert_config.hidden_size  hidden_size(第二个参数)= 跟Linear 的第一个参数对上\n",
    "        # 尝试下双向LSTM\n",
    "        self.lm_flag = lm_flag\n",
    "        self.lstm = nn.LSTM(bert_config.hidden_size, lstm_hidden_size,\n",
    "                            num_layers=1, bidirectional=True, dropout=0.3, batch_first=True)\n",
    "        self.clf = nn.Linear(256,bert_config.num_labels + 2)\n",
    "        self.layer_norm = nn.LayerNorm(lstm_hidden_size * 2)\n",
    "        self.crf = CRF(target_size=bert_config.num_labels, average_batch=True, use_cuda=True)\n",
    "        \n",
    "    def forward(self,input_ids,masks):\n",
    "        batch_size = input_ids.size(0)\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        outputs = self.bert(input_ids, token_type_ids=None,\n",
    "                     attention_mask=masks)\n",
    "        # 方案一：\n",
    "#         embeds = outputs[0]\n",
    "        \n",
    "        #方案二：倒数第二层hidden_states 的shape\n",
    "        # bert_config的设置\n",
    "        all_hidden_states, all_attentions = outputs[-2:]\n",
    "#         print('all_hidden_states',all_hidden_states.shape)\n",
    "        embeds = all_hidden_states[-2]\n",
    "#         print('embeds',embeds.shape)\n",
    "\n",
    "        lstm_out,hidden = self.lstm(embeds)\n",
    "        lstm_out= lstm_out.contiguous().view(-1, 128*2)\n",
    "        if self.lm_flag:\n",
    "            lstm_out = self.layer_norm(lstm_out)\n",
    "        logits = self.clf(lstm_out)\n",
    "        logits = logits.contiguous().view(batch_size, seq_length, -1)\n",
    "        return logits\n",
    "    \n",
    "    def loss(self,logits,mask,tag):\n",
    "        loss_value = self.crf.neg_log_likelihood_loss(logits,mask,tag)\n",
    "        bs = logits.size(0)\n",
    "        loss_value  /= float(bs)\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert_CRF(base_path,num_labels = len(tag2id),lm_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # default is 5e-5\n",
    "                  eps = 1e-8 # default is 1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                           num_warmup_steps = 0,\n",
    "                                           num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2label(id2tag,data,lengths):\n",
    "    new = []\n",
    "    for i,line in enumerate(data):\n",
    "        tmp = [id2tag[word] for word in line]\n",
    "        tmp = tmp[1:1 + lengths[i]]    \n",
    "        new.append(tmp)\n",
    "    return new\n",
    "\n",
    "def get_entities(tags):\n",
    "    start, end = -1, -1\n",
    "    prev = 'O'\n",
    "    entities = []\n",
    "    n = len(tags)\n",
    "    tags = [tag.split('-')[1] if '-' in tag else tag for tag in tags]\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag != 'O':\n",
    "            if prev == 'O':\n",
    "                start = i\n",
    "                prev = tag\n",
    "            elif tag == prev:\n",
    "                end = i\n",
    "                if i == n -1 :\n",
    "                    entities.append((start, i))\n",
    "            else:\n",
    "                entities.append((start, i - 1))\n",
    "                prev = tag\n",
    "                start = i\n",
    "                end = i\n",
    "        else:\n",
    "            if start >= 0 and end >= 0:\n",
    "                entities.append((start, end))\n",
    "                start = -1\n",
    "                end = -1\n",
    "                prev = 'O'\n",
    "    return entities\n",
    "\n",
    "def measure(preds,trues,lengths,id2tag):\n",
    "    correct_num = 0\n",
    "    predict_num = 0\n",
    "    truth_num = 0\n",
    "    pred = trans2label(id2tag,preds,lengths)\n",
    "    true = trans2label(id2tag,trues,lengths)\n",
    "    assert len(pred) == len(true)\n",
    "    for p,t in zip(pred,true):\n",
    "        pred_en = get_entities(p)\n",
    "        true_en = get_entities(t)\n",
    "#         print('pred_en',pred_en)\n",
    "#         print('true_en',true_en)\n",
    "        correct_num += len(set(pred_en) & set(true_en))\n",
    "        predict_num += len(set(pred_en))\n",
    "        truth_num += len(set(true_en))\n",
    "    precision = correct_num / predict_num if predict_num else 0\n",
    "    recall = correct_num / truth_num if truth_num else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_grad_norm = 1.0\n",
    "\n",
    "tra_loss,train_steps = 0.0,0\n",
    "dev_loss,dev_steps = 0.0,0\n",
    "\n",
    "start = time.time()\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    for step ,batch in enumerate(train_dataloader):\n",
    "        input_ids,masks,labels= (i.to(device) for i in batch)\n",
    "        outputs = model(input_ids,masks)\n",
    "        loss = model.loss(outputs,masks,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        tra_loss += loss.item()\n",
    "        train_steps += 1\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 30 == 0:\n",
    "            print(\"epoch :{},step :{} ,Train loss: {}\".format(i,step,tra_loss/train_steps))\n",
    "    \n",
    "    print(\"Training Loss of epoch {}:{}\".format(i,tra_loss / train_steps))\n",
    "    \n",
    "    model.eval()\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    for step ,batch in enumerate(valid_dataloader):\n",
    "        input_ids,masks,labels = (i.to(device) for i in batch)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids,masks)\n",
    "            loss = model.loss(logits,masks,labels)\n",
    "            path_score, best_path = model.crf(logits, input_ids.bool())\n",
    "            \n",
    "            dev_loss += loss.item()\n",
    "            dev_steps += 1\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(\"epoch :{},step :{} ,Dev loss: {}\".format(i,step,dev_loss/dev_steps))\n",
    " \n",
    "        best_path = best_path.detach().cpu().numpy().tolist()\n",
    "        predictions.extend(best_path)\n",
    "        true_labels.extend(labels.to('cpu').numpy().tolist())\n",
    "    f1, precision, recall = measure(predictions,true_labels,dev_lengths,id2tag)\n",
    "    print('epoch {} : Acc : {},Recall : {},F1 :{}'.format(i,precision,recall,f1))\n",
    "end = time.time()\n",
    "print('Training Time:',end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens,test_labels = load_data(base,test_path)\n",
    "test_ids,test_tags,test_masks,test_lengths = gen_features(test_tokens,test_labels,tokenizer,tag2id,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = torch.tensor(test_ids)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_ids, test_masks, test_tags)\n",
    "# test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_(preds,trues,lengths,id2tag,test_tokens):\n",
    "    correct_num = 0\n",
    "    predict_num = 0\n",
    "    truth_num = 0\n",
    "    pred = trans2label(id2tag,preds,lengths)\n",
    "    true = trans2label(id2tag,trues,lengths)\n",
    "    assert len(pred) == len(true)\n",
    "    for i,(p,t) in enumerate(zip(pred,true)):\n",
    "        pred_en = get_entities(p)\n",
    "        true_en = get_entities(t)\n",
    "        print('pred_en',pred_en)\n",
    "        print('true_en',true_en)\n",
    "        print(test_tokens[i])\n",
    "        print('***********************')\n",
    "        correct_num += len(set(pred_en) & set(true_en))\n",
    "        predict_num += len(set(pred_en))\n",
    "        truth_num += len(set(true_en))\n",
    "    precision = correct_num / predict_num if predict_num else 0\n",
    "    recall = correct_num / truth_num if truth_num else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_pre,test_true = [],[]\n",
    "for batch in test_dataloader:\n",
    "    input_ids,masks,labels = (i.to(device) for i in batch)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids,masks)\n",
    "        path_score, test_best_path = model.crf(logits, input_ids.bool())\n",
    "\n",
    "    test_best_path = test_best_path.detach().cpu().numpy().tolist()\n",
    "    test_pre.extend(test_best_path)\n",
    "    test_true.extend(labels.to('cpu').numpy().tolist())\n",
    "\n",
    "test_f1, test_precision, test_recall = measure_(test_pre,test_true,test_lengths,id2tag,test_tokens)\n",
    "# test_pred = trans2label(id2tag,test_pre,test_lengths)\n",
    "# test_trues = trans2label(id2tag,test_true,test_lengths)\n",
    "# measure_1(test_pre,test_true,test_lengths,id2tag,test_tokens)\n",
    "print('Test Acc : {},Recall : {},F1 :{}'.format(test_precision,test_recall,test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.bert.named_parameters())\n",
    "# list(model.lstm.named_parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
