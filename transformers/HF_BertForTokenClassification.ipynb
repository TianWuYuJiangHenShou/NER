{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# import pretraining_args as args\n",
    "from transformers import BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer,BertConfig,BertForTokenClassification,BertModel\n",
    "import time,datetime\n",
    "from sklearn.metrics import precision_score,classification_report,f1_score,recall_score\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import AlbertConfig, AlbertModel,AlbertForTokenClassification\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from crf import CRF\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/root/yy/data/ResumeNER'\n",
    "base_path = '../berts/bert-base-transformers'\n",
    "train_path = 'train.char.bmes'\n",
    "dev_path = 'dev.char.bmes'\n",
    "test_path = 'test.char.bmes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base,train_path):\n",
    "    full = os.path.join(base,train_path)\n",
    "    with open(full,'r',encoding='utf-8')as f:\n",
    "        data = f.readlines()\n",
    "    tokens,labels = [],[]\n",
    "    token,label = [],[]\n",
    "    for line in data:\n",
    "        line= line.strip().replace(\"\\n\",'')\n",
    "        if len(line.split(' ')) == 2:\n",
    "            token.append(line.split(' ')[0])\n",
    "            label.append(line.split(' ')[1])\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "            token,label = [],[]\n",
    "    return tokens,labels\n",
    "\n",
    "def trans2id(labels):\n",
    "    tag_set = set()\n",
    "    for line in labels:\n",
    "        for label  in line:\n",
    "            if label not in tag_set:\n",
    "                tag_set.add(label)\n",
    "    tag_set.add('[CLS]')\n",
    "    tag_set.add('[SEP]')\n",
    "    tag_set = list(tag_set)\n",
    "    idx = [i for i in range(len(tag_set))]\n",
    "    tag2id = dict(zip(tag_set,idx))\n",
    "    id2tag = dict(zip(idx,tag_set))\n",
    "    return tag2id,id2tag\n",
    "\n",
    "def gen_features(tokens,labels,tokenizer,tag2id,max_len):\n",
    "    input_ids,tags,masks,lengths = [],[],[],[]\n",
    "    for i,(token,label) in enumerate(zip(tokens,labels)):\n",
    "        lengths.append(len(token))\n",
    "        if len(token) >= max_len - 2:\n",
    "            token = token[0:max_len - 2]\n",
    "            label = labels[i][0:max_len - 2]\n",
    "        mask = [1] * len(token)\n",
    "        \n",
    "        token = '[CLS] ' + ' '.join(token) + ' [SEP]'\n",
    "        tokenized_text = tokenizer.tokenize(token)\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        label = [tag2id['[CLS]']] + [tag2id[i] for i in label] + [tag2id['[SEP]']]\n",
    "        mask = [0] + mask + [0]\n",
    "        # padding\n",
    "        if len(input_id) < max_len:\n",
    "            input_id = input_id + [0] * (max_len - len(input_id))\n",
    "            label = label + [tag2id['O']] * (max_len - len(label))\n",
    "            mask = mask + [0] * (max_len - len(mask))\n",
    "        \n",
    "        assert len(input_id) == max_len\n",
    "        assert len(label) == max_len\n",
    "        assert len(mask) == max_len\n",
    "         \n",
    "        input_ids.append(input_id)\n",
    "        tags.append(label)\n",
    "        masks.append(mask)\n",
    "    return input_ids,tags,masks,lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "bs = 32\n",
    "tokenizer = BertTokenizer.from_pretrained('../berts/bert-base-transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tokens,train_labels = load_data(base,train_path)\n",
    "tag2id,id2tag = trans2id(train_labels)\n",
    "train_ids,train_tags,train_masks,train_lengths = gen_features(train_tokens,train_labels,tokenizer,tag2id,max_len)\n",
    "\n",
    "dev_tokens,dev_labels = load_data(base,dev_path)\n",
    "dev_ids,dev_tags,dev_masks,dev_lengths = gen_features(dev_tokens,dev_labels,tokenizer,tag2id,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file(os.path.join(base_path,'config.json'))\n",
    "config.num_labels = len(tag2id)\n",
    "bert = BertForTokenClassification.from_pretrained(os.path.join(base_path,'pytorch_model.bin'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ids = torch.tensor(train_ids)\n",
    "train_tags = torch.tensor(train_tags)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "# train_lengths = torch.tensor(train_lengths)\n",
    "\n",
    "dev_ids = torch.tensor(dev_ids)\n",
    "dev_tags = torch.tensor(dev_tags)\n",
    "dev_masks = torch.tensor(dev_masks)\n",
    "# dev_lengths = torch.tensor(dev_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_ids, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(dev_ids, dev_masks, dev_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert.parameters(),\n",
    "                  lr = 5e-5, # default is 5e-5\n",
    "                  eps = 1e-8 # default is 1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                           num_warmup_steps = 0,\n",
    "                                           num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2label(id2tag,data,lengths):\n",
    "    new = []\n",
    "    for i,line in enumerate(data):\n",
    "        tmp = [id2tag[word] for word in line]\n",
    "        tmp = tmp[1:1 + lengths[i]]    \n",
    "        new.append(tmp)\n",
    "    return new\n",
    "\n",
    "def get_entities(tags):\n",
    "    start, end = -1, -1\n",
    "    prev = 'O'\n",
    "    entities = []\n",
    "    n = len(tags)\n",
    "    tags = [tag.split('-')[1] if '-' in tag else tag for tag in tags]\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag != 'O':\n",
    "            if prev == 'O':\n",
    "                start = i\n",
    "                prev = tag\n",
    "            elif tag == prev:\n",
    "                end = i\n",
    "                if i == n -1 :\n",
    "                    entities.append((start, i))\n",
    "            else:\n",
    "                entities.append((start, i - 1))\n",
    "                prev = tag\n",
    "                start = i\n",
    "                end = i\n",
    "        else:\n",
    "            if start >= 0 and end >= 0:\n",
    "                entities.append((start, end))\n",
    "                start = -1\n",
    "                end = -1\n",
    "                prev = 'O'\n",
    "    return entities\n",
    "\n",
    "def measure(preds,trues,lengths,id2tag):\n",
    "    correct_num = 0\n",
    "    predict_num = 0\n",
    "    truth_num = 0\n",
    "    pred = trans2label(id2tag,preds,lengths)\n",
    "    true = trans2label(id2tag,trues,lengths)\n",
    "    assert len(pred) == len(true)\n",
    "    for p,t in zip(pred,true):\n",
    "        pred_en = get_entities(p)\n",
    "        true_en = get_entities(t)\n",
    "        correct_num += len(set(pred_en) & set(true_en))\n",
    "        predict_num += len(set(pred_en))\n",
    "        truth_num += len(set(true_en))\n",
    "    precision = correct_num / predict_num if predict_num else 0\n",
    "    recall = correct_num / truth_num if truth_num else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_grad_norm = 1.0\n",
    "\n",
    "tra_loss,steps = 0.0,0\n",
    "for i in range(epochs):\n",
    "    bert.train()\n",
    "    for step ,batch in enumerate(train_dataloader):\n",
    "        input_ids,masks,labels= (i.to(device) for i in batch)\n",
    "        outputs = bert(input_ids,attention_mask = masks,labels = labels)\n",
    "        loss, scores = outputs[:2]\n",
    "        loss.backward()\n",
    "        \n",
    "        tra_loss += loss\n",
    "        steps += 1\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(parameters=bert.parameters(), max_norm=max_grad_norm)\n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 30 == 0:\n",
    "            print(\"epoch :{},step :{} ,Train loss: {}\".format(i,step,tra_loss/steps))\n",
    "    \n",
    "    print(\"Training Loss of epoch {}:{}\".format(i,tra_loss / steps))\n",
    "    \n",
    "    bert.eval()\n",
    "    dev_loss = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        input_ids,masks,labels = (i.to(device) for i in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids,attention_mask = masks)\n",
    "            #scores:(batch_size, sequence_length, config.num_labels) -> before softmax\n",
    "            scores = outputs[0]\n",
    "            \n",
    "        scores = scores.detach().cpu().numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(scores, axis=2)])\n",
    "        true_labels.extend(labels.to('cpu').numpy().tolist())\n",
    "#         lengths = lengths.detach().cpu().numpy().tolist()\n",
    "#     dev_lengths = dev_lengths.detach().cpu().numpy()\n",
    "    f1, precision, recall = measure(predictions,true_labels,dev_lengths,id2tag)\n",
    "    print('epoch {} : Acc : {},Recall : {},F1 :{}'.format(i,precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens,test_labels = load_data(base,test_path)\n",
    "test_ids,test_tags,test_masks,test_lengths = gen_features(test_tokens,test_labels,tokenizer,tag2id,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = torch.tensor(test_ids)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_ids, test_masks, test_tags)\n",
    "# test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert.eval()\n",
    "test_pre,test_true = [],[]\n",
    "for batch in test_dataloader:\n",
    "    input_ids,masks,labels = (i.to(device) for i in batch)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(input_ids,attention_mask = masks)\n",
    "        #scores:(batch_size, sequence_length, config.num_labels) -> before softmax\n",
    "        scores = outputs[0]\n",
    "\n",
    "    scores = scores.detach().cpu().numpy()\n",
    "    test_pre.extend([list(p) for p in np.argmax(scores, axis=2)])\n",
    "    test_true.extend(labels.to('cpu').numpy().tolist())\n",
    "\n",
    "test_f1, test_precision, test_recall = measure(test_pre,test_true,test_lengths,id2tag)\n",
    "# test_pred = trans2label(id2tag,test_pre,test_lengths)\n",
    "# test_trues = trans2label(id2tag,test_true,test_lengths)\n",
    "# measure_1(test_pre,test_true,test_lengths,id2tag,test_tokens)\n",
    "print('Test Acc : {},Recall : {},F1 :{}'.format(test_precision,test_recall,test_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
